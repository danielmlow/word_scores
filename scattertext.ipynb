{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "100fba44-860d-4dc9-9045-0e07785b441a",
   "metadata": {},
   "source": [
    "# What words and phrases distinguish two groups of documents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f8131-07c1-4394-a207-3cc946aa86eb",
   "metadata": {},
   "source": [
    "Author: Daniel Low\n",
    "Using the scattertext package\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4abb22d-695f-4c83-85c4-6357c2b32d87",
   "metadata": {},
   "source": [
    "Paper: Kessler, J. (2017, July). Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ. In Proceedings of ACL 2017, System Demonstrations (pp. 85-90).\n",
    "\n",
    "Tutorials:\n",
    "- https://github.com/JasonKessler/scattertext\n",
    "- https://colab.research.google.com/drive/1snxAP8X6EIDi42FugJ_h5U-fBGDCqtyS#scrollTo=Zmo4wLW36Xuq \n",
    "\n",
    "Parameter descriptions are not well described, but review tutorials and\n",
    "- https://github.com/JasonKessler/scattertext/blob/master/scattertext/__init__.py\n",
    "- The \"compaction\" process allows users to eliminate terms which may not be associated with a category using a variety of feature selection methods.  The issue with this is that the terms eliminated during the selection process are not taken into account when scaling term positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d83c87-d887-42f6-a767-389fb0459fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib==3.6.0 spacy==3.6.1 scattertext==0.1.19 pytextrank==3.2.5 textalloc==0.0.3\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f61484-2998-4bd3-921c-0136b71296b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b13cb38-b958-4686-89c6-ea37cd130329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q pytextrank==3.2.5\n",
    "import pytextrank, spacy\n",
    "import scattertext as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe90da98-3411-4d26-a674-5429761869df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eebc4c-22a4-497b-b615-7ec2aca0cc84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215172cb-957d-4888-8ab7-14a485068fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scattertext import SampleCorpora, PhraseMachinePhrases, dense_rank, RankDifference, AssociationCompactor, produce_scattertext_explorer\n",
    "from scattertext.CorpusFromPandas import CorpusFromPandas\n",
    "import scattertext as st\n",
    "\n",
    "on_google_drive = False\n",
    "\n",
    "if on_google_drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    input_dir = '/content/drive/MyDrive/datum/rallypoint_suicide_detection/data/input/'\n",
    "    output_dir = '/content/drive/MyDrive/datum/rallypoint_suicide_detection/data/output/'\n",
    "else:\n",
    "    input_dir = './data/input/final_datasets/'\n",
    "    output_dir = './data/output/semantic_analysis/'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f976c56d-18f0-4bf9-a907-4709b685fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(input_dir + 'train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dee39d-604a-47c2-84ff-5f6547ab5f91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_col_name = 'content'\n",
    "group_col_name = 'Group'\n",
    "category = 'suicidal'\n",
    "not_category = 'nonsuicidal'\n",
    "minimum_term_frequency=7\n",
    "compaction = 2000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc361324-21e2-49dd-b573-5f904e7f4fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train[group_col_name] = [category if n==1 else not_category for n in train['label'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e75db-087a-401c-a115-a6e523140421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7afd81-4293-41a8-933b-1e601126778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LogOddsRatioSmoothedZScorePrior:\n",
    "    def __init__(self, prior, prior_scale):\n",
    "        self.prior = prior\n",
    "        self.prior_scale = prior_scale\n",
    "    def get_scores(self, a, b): \n",
    "        ap = a + self.prior * self.prior_scale*sum(a)/sum(self.prior.values)\n",
    "        bp = b + self.prior * self.prior_scale*sum(b)/sum(self.prior.values)\n",
    "        lor = (np.log(ap/(np.sum(ap) - ap)) - np.log(bp/(np.sum(bp) - bp)))\n",
    "        lorstd = 1./ap + 1./(np.sum(ap) - ap) + 1./bp + 1./(np.sum(bp) - bp)\n",
    "        return lor/np.sqrt(lorstd)\n",
    "        \n",
    "    def get_name(self): \n",
    "        return 'Log-Odds-Ratio w/ Informative Dirichlet Prior Z-Score'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def return_method(method_name, priors = None, odds_ratio_prior_n = 10):\n",
    "\n",
    "    if method_name == 'fscore':\n",
    "        return st.ScaledFScorePresetsNeg1To1(beta=1, scaler_algo='normcdf')\n",
    "    elif method_name == 'rank_difference':\n",
    "        return RankDifference()\n",
    "    # elif method_name == 'dense_rank':\n",
    "    #     return st.Scalers.dense_rank\n",
    "    elif method_name == 'odds_ratio':\n",
    "        return st.LogOddsRatioInformativeDirichletPrior(priors, odds_ratio_prior_n, 'class-size')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a913e5-7397-4a2f-8def-60c8bcd003f7",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb9144c-fa1a-472a-b50c-19202673fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "# Unigram corpus \n",
    "# =======================================\n",
    "df_parsed = train.assign(\n",
    "    parse=lambda df: df[text_col_name].apply(st.whitespace_nlp_with_sentences)\n",
    ")\n",
    "\n",
    "\n",
    "corpus_unigram = st.CorpusFromParsedDocuments(\n",
    "    df_parsed, category_col=group_col_name, parsed_col='parse'\n",
    ").build().get_unigram_corpus().compact(st.AssociationCompactor(compaction))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def return_corpus(ngram):\n",
    "    if ngram == 'word':\n",
    "        return corpus_unigram\n",
    "    \n",
    "    elif ngram == 'phrases':\n",
    "        return corpus_phrases\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e72889-1a6e-4359-bfe6-04d28b2992d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "compaction_phrases = 4000\n",
    "\n",
    "# Using Phrase Machine phrase detector\n",
    "# =======================================\n",
    "corpus_phrases = (CorpusFromPandas(train,\n",
    "                           category_col=group_col_name,\n",
    "                           text_col=text_col_name,\n",
    "                           feats_from_spacy_doc=PhraseMachinePhrases(),\n",
    "                           # nlp=spacy.load('en', parser=False)\n",
    "                              nlp = spacy.load(\"en_core_web_sm\")\n",
    "                             ).build().compact(AssociationCompactor(compaction_phrases)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6d301-060a-44c3-a236-3ba4d2c76c21",
   "metadata": {},
   "source": [
    "# Word scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d37af2-ebfe-4a92-b8b2-299150653658",
   "metadata": {},
   "source": [
    "## Words scores (axes are term frequency in each group, color-coded by F-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35177cbd-9b58-4910-b0d7-ea329430cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 'word' # {'word', 'phrases'}\n",
    "score_method = 'fscore' # {'fscore','rank_difference', 'dense_rank', 'odds_ratio'}\n",
    "axes = 'freq' # {'freq', 'score'}\n",
    "filename = f'{ngram}_method-{score_method}_axes-{axes}_compaction-{compaction}-freq-{minimum_term_frequency}'\n",
    "\n",
    "html = st.produce_scattertext_explorer(\n",
    "    return_corpus(ngram),\n",
    "    category=category, category_name=category, not_category_name=not_category,\n",
    "    minimum_term_frequency=minimum_term_frequency, pmi_threshold_coefficient=0,\n",
    "    width_in_pixels=1000, \n",
    "    transform=st.Scalers.percentile_alphabetical,\n",
    "    # metadata=corpus.get_df()['type'],\n",
    "    # transform = default is percentile_alphabetical?, #st.Scalers.scale_center_zero_abs, #{dense_rank, st.Scalers.scale_center_zero_abs(vec)}\n",
    "                                       # https://github.com/JasonKessler/scattertext/blob/b41e3a875faf6dd886e49e524345202432db1b21/scattertext/Scalers.py#L217\n",
    "    # scores\n",
    "    term_scorer=return_method(score_method)\n",
    ")\n",
    "open(output_dir+filename+'.html', 'w').write(html)\n",
    "corpus_unigram_clean = corpus.remove_terms(terms = ['groupforsuicidalactivedutyandforsuicidalveterans'])\n",
    "\n",
    "# just copy it but add return_scatterplot_structure=True to output the structure\n",
    "scattertext_structure = produce_scattertext_explorer(\n",
    "    \n",
    "    # return_corpus(ngram),\n",
    "    corpus_unigram_clean,\n",
    "                                    category=category,\n",
    "                                    category_name=category,\n",
    "                                    not_category_name=not_category,\n",
    "                                    minimum_term_frequency=minimum_term_frequency,\n",
    "                                    pmi_threshold_coefficient=0,\n",
    "                                    transform=st.Scalers.percentile_alphabetical,\n",
    "                                    # metadata=corpus.get_df()['speaker'],\n",
    "                                    term_scorer=return_method(score_method),\n",
    "                                    width_in_pixels=1000,\n",
    "    x_label = 'Log frequency in '+not_category+' posts',\n",
    "    y_label = 'Log frequency in '+category+' posts',\n",
    "    \n",
    "                                 return_scatterplot_structure=True,\n",
    "                                                    )\n",
    "\n",
    "\n",
    "\n",
    "fig = st.produce_scattertext_pyplot(scattertext_structure)\n",
    "fig.savefig(output_dir+f'{filename}.tiff', format='tiff', dpi = 150)\n",
    "fig.savefig(output_dir+f'{filename}.png', format='png', dpi = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8f003-2bd1-4c02-b0f6-fe34c9da693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispersion = st.Dispersion(return_corpus(ngram))\n",
    "dispersion_df = dispersion.get_df()\n",
    "display(dispersion_df)\n",
    "\n",
    "term_category_scores = return_corpus(ngram).get_metadata_freq_df('')\n",
    "print(term_category_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd4be4-f212-410e-8733-4de26845ddbc",
   "metadata": {},
   "source": [
    "### Save as figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35322424-ca96-4e98-a5ff-e124d6778d0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[n for n in corpus.get_terms() if n.startswith('groupfor')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5e943-5bfc-4c34-9594-bcf5310d858b",
   "metadata": {},
   "source": [
    "## Word scores (axes are F-score as a function of term frequency in corpus, color-coded by F-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f3295-d9a7-4dea-8023-04dcda4bc37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 'word' # {'word', 'phrases'}\n",
    "score_method = 'fscore' # {'fscore','rank_difference', 'dense_rank', 'odds_ratio'}\n",
    "axes = 'score' # {'freq', 'score'}\n",
    "filename = f'{ngram}_method-{score_method}_axes-{axes}_compaction-{compaction}-freq-{minimum_term_frequency}'\n",
    "\n",
    "html = st.produce_fightin_words_explorer(\n",
    "    corpus_unigram_clean,\n",
    "    category=category,\n",
    "    category_name=category,\n",
    "    not_category_name=not_category,\n",
    "    not_categories=[not_category],\n",
    "    neutral_categories=[],\n",
    "    transform=st.Scalers.percentile_alphabetical,\n",
    "    minimum_term_frequency=minimum_term_frequency,\n",
    "    term_scorer=st.ScaledFScorePresetsNeg1To1(beta=1, scaler_algo='normcdf'),\n",
    "    # metadata = rdf['movie_name'],\n",
    "    grey_threshold=0.1\n",
    ")\n",
    "\n",
    "open(output_dir+filename+'.html', 'w').write(html)\n",
    "\n",
    "\n",
    "# just copy it but add return_scatterplot_structure=True to output the structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac49e78-5b26-42e6-96ce-d3900468f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scattertext_structure._y_axis_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4862cc7-88d4-4622-9455-9ea785dadf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispersion = st.Dispersion(corpus_unigram_clean).get_df()\n",
    "dispersion['Frequency'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0399723b-3912-4110-b290-b76a6f0a3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed = np.array([n.text for n in corpus_unigram_clean.get_df()['parse'].values])\n",
    "# parsed\n",
    "# print(len(parsed), 'documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e123dc-9c20-4e1b-b405-5eed359fc166",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tokens = []\n",
    "# for n in parsed:\n",
    "#     try: tokens.append(n.replace('\\n',' ').replace('\\t', ' '.split(' '))\n",
    "#     except: \n",
    "#         print(n)\n",
    "# tokens = np.unique([n for i in tokens for n in i])\n",
    "# counts_df = pd.DataFrame(counts, index = ['counts']).T\n",
    "# counts_df[counts_df['counts']>=2] #none, because I need to parse tokens correctly removing punctuation, lower case\n",
    "# counts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8112af3-0392-4122-862f-3e1b2df762aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scattertext_structure = st.produce_fightin_words_explorer(\n",
    "corpus_unigram_clean,\n",
    "    category=category,\n",
    "    category_name=category,\n",
    "    not_category_name=not_category,\n",
    "    not_categories=[not_category],\n",
    "    neutral_categories=[],\n",
    "    transform=st.Scalers.percentile_alphabetical,\n",
    "    minimum_term_frequency=minimum_term_frequency,\n",
    "    term_scorer=st.ScaledFScorePresetsNeg1To1(beta=1, scaler_algo='normcdf'),\n",
    "    # metadata = rdf['movie_name'],\n",
    "    grey_threshold=0.1,\n",
    "\n",
    "    \n",
    "    # y_axis_values = [-1,0,1],\n",
    "    return_scatterplot_structure=True\n",
    ")\n",
    "# Change plotting params add '_' at the beginning:\n",
    "scattertext_structure._x_label = 'Log frequency in entire corpus'\n",
    "scattertext_structure._y_label = 'Non-suicidal vs. suicidal (Scaled F-Score)'\n",
    "\n",
    "# Change plotting params: I think it requires 3 values\n",
    "log_freq_distr = dispersion['Frequency'].values\n",
    "min_freq = int(np.min(log_freq_distr))\n",
    "max_freq = int(np.max(log_freq_distr))\n",
    "scattertext_structure._x_axis_labels=[min_freq, '',max_freq]\n",
    "# https://github.com/JasonKessler/scattertext/blob/b41e3a875faf6dd886e49e524345202432db1b21/scattertext/viz/PyPlotFromScattertextStructure.py#L149\n",
    "scattertext_structure._y_axis_labels=[-1,0,1]#[f'{not_category} (-1)','0',f'{category} (1)']\n",
    "\n",
    "\n",
    "\n",
    "fig = st.produce_scattertext_pyplot(scattertext_structure, textsize = 8, dpi=300)\n",
    "# plt.xticks(ticks = [-1,0,1],labels=[-1,0,1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(output_dir+f'{filename}.tiff', format='tiff', dpi = 150)\n",
    "fig.savefig(output_dir+f'{filename}.png', format='png', dpi = 150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5390003-590b-4279-a9ab-b48f95346b36",
   "metadata": {},
   "source": [
    "### List sentences with top words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18561d-dfd4-4fb3-a4e8-99acc45cf61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n=100\n",
    "term_freq_df = corpus_unigram.get_term_freq_df()\n",
    "term_freq_df[f'{category} score'] = corpus.get_scaled_f_scores(category)\n",
    "top_words_category = list(term_freq_df.sort_values(by=f'{category} score', ascending=False).index[:n])\n",
    "top_words_category_score = corpus_unigram.get_scaled_f_scores(category)[:n]\n",
    "# top_words_not_category = list(term_freq_df.sort_values(by=f'{category} score', ascending=False).index[::-1][:n])\n",
    "# top_words_not_category_score = corpus_unigram.get_scaled_f_scores(category)[::-1][:n]\n",
    "\n",
    "df_i = corpus_unigram.get_df()\n",
    "df_category = df_i[df_i[group_col_name]==category]\n",
    "df_not_category = df_i[df_i[group_col_name]==not_category]\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "def return_sample_word_in_docs(df, words, text_col_name = 'content', sample_n = 5):\n",
    "    words_to_docs = {}\n",
    "    words_to_docs_windows = {}\n",
    "    for word in words:\n",
    "        if word == 'i':\n",
    "            word = 'I'\n",
    "        elif word == 'm':\n",
    "            word = \"I'm\"\n",
    "        # elif word  == \n",
    "        docs_with_word = df[df[text_col_name].str.contains(word)][text_col_name].sample(n=sample_n)\n",
    "        assert docs_with_word.shape[0] == sample_n\n",
    "        docs_with_word = docs_with_word.values\n",
    "        docs_with_word = [n.replace('\\n', ' ') for n in docs_with_word]\n",
    "        words_to_docs[word] = '\\n'.join(docs_with_word)\n",
    "        \n",
    "        regex_str = r'(?:\\b\\S+\\s*){,7}\\b'+ re.escape(word) +r'\\b(?:\\s*\\S+\\s*?){,7}'\n",
    "        \n",
    "\n",
    "        # from each post, return n words before and after target word.\n",
    "        docs_with_word_windows = [re.findall(regex_str, doc) for doc in docs_with_word]\n",
    "        \n",
    "        docs_with_word_windows = [n for i in docs_with_word_windows for n in i]\n",
    "        words_to_docs_windows[word] = '\\n'.join(docs_with_word_windows)\n",
    "    return words_to_docs, words_to_docs_windows\n",
    "        \n",
    "words_to_docs_category_d, words_to_docs_windows_category_d  = return_sample_word_in_docs(df_category, top_words_category, sample_n = 5)\n",
    "# docs_not_category_d = return_sample_word_in_docs(df_not_category, top_words_not_category, sample = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0bfc89-408b-4117-9ad8-914521b900d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs_with_word_windows = [re.findall(regex_str, doc)[0] for doc in docs_with_word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02286c27-7dcd-4a2f-815c-b012c79beaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7555827-f273-41e8-a635-f4ef6fcc9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_examples = pd.DataFrame(words_to_docs_windows_category_d, index = ['Examples']).T\n",
    "word_examples.to_csv(output_dir+filename+f'_examples_top_{n}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563e3b2f-c910-4d2b-8936-145c08f7f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 15\n",
    "\n",
    "# for word in top_words[:n]:\n",
    "#     print(words_to_docs_windows_category_d.get(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05909df8-a915-42af-9de6-251f6fbc43bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scattertext_structure._visualization_data.word_dict[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165d8a6-e2af-4ea3-b42c-3a13defa68a7",
   "metadata": {},
   "source": [
    "## Word scores (axes are weighted log odds ratio as a function of term frequency in corpus, color-coded by weighted log odds ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e071d81c-8cf4-4090-97c5-cb75c2b185f1",
   "metadata": {},
   "source": [
    "\n",
    "### Log-Odds-Ratio with Informative Dirichlet Prior Z-Score\n",
    "### Prefered, currently very popular in NLP and CSS literature, flaws exist\n",
    "### Note: this formulation is modified slightly from Monroe et al. 2008.\n",
    "\n",
    "We know, intuitively, words which which are used very frequently require much more evidence to be significantly associated with a class than less frequent words.  For example, words like \"best\" and \"entertaining\" in the above example should be scored lower than \"and\" and \"of\".  \n",
    "\n",
    "The approach developed by Monroe et. al is to more aggressively smooth terms based on their frequency in a background corpus.\n",
    "\n",
    "The downside is of this approach is that obtaining a background corpus may be difficult. Lists of background term frequencies, especially for 2+grams, can be very large and unwieldy. Moreover, term frequency lists can become outdated very quickly, especially wrt to politics, pop culture, and technology.\n",
    "\n",
    "We construct two vectors of background term counts for each category, $\\alpha_a$ and $\\alpha_b$.  The priors are based on a background set of word frequencies, referred to as $y_{c}$  These are ideally in the same domain as the documents being studied, although could come from a general list of word frequencies.  The background counts are normalized to the size of their respective categories. The hyperparameter $\\sigma$ scales the background counts to a multiple of their category size. In other words, as $\\sigma$ increases, the regularization increases, and the odds ratio tends more toward zero.\n",
    "\n",
    "The $\\sigma$ I've found most useful is 10, which I include in Scattertext.\n",
    "\n",
    "$$ \\alpha_a = \\sigma \\cdot n_a \\cdot \\frac{y_{ci}}{n_c} $$\n",
    "\n",
    "$$ \\alpha_b = \\sigma \\cdot n_b \\cdot \\frac{y_{ci}}{n_c} $$\n",
    "\n",
    "\n",
    "The $\\alpha$s can then be used analogously to find the Z-Score of the LOR.\n",
    "\n",
    "$$ \\mbox{LOR}(\\mbox{term}_i, \\mbox{category}_a, \\mbox{category}_b) = \\log \\frac{y_{ai} + \\alpha_a}{n_{a} + \\alpha_a \\cdot |y| - y_{ai} - \\alpha_a} - \\log \\frac{y_{bi} + \\alpha_b}{n_{b} + \\alpha_b \\cdot |y| - y_{bi} - \\alpha_b} $$\n",
    "\n",
    "$$ \\mbox{LOR-SE}(\\mbox{term}_i, \\mbox{category}_a, \\mbox{category}_b) = \\frac{1}{y_{ai} + \\alpha_a} + \\frac{1}{n_{a} + \\alpha_a \\cdot |y| - y_{ai} - \\alpha_a} + \\frac{1}{y_{bi} + \\alpha_b}+ \\frac{1}{n_{b} + \\alpha_b \\cdot |y| - y_{bi} - \\alpha_b} $$\n",
    "\n",
    "Monroe et al. used a different approach to finding $\\alpha$, scaling the sum of the peusdocounts to the mean number of words in a document.\n",
    "\n",
    "$$ \\alpha_a = \\alpha_b = \\overline{|d_{y}|} \\frac{y_{ci}}{\\sum n_{c}} $$\n",
    "\n",
    "Jurafsky et al. suggested using raw background corpus counts as $\\alpha$.\n",
    "\n",
    "$$ \\alpha_a = \\alpha_b = y_c $$\n",
    "\n",
    "In the following example, we use the plot description as a background corpus using the first method.\n",
    "\n",
    "* Dan Jurafsky, Victor Chahuneau, Bryan Routledge, and Noah Smith. Narrative framing of consumer sentiment in online restaurant reviews. First Monday. 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46646f1f-6975-4b11-a224-ffc78526fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors_unigram = (st.PriorFactory(corpus_unigram, \n",
    "                  category=category, \n",
    "                  not_categories=[not_category],\n",
    "                  starting_count=0.01)\n",
    "  .use_neutral_categories()\n",
    "  .get_priors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee82df26-e5f9-4c77-947a-ea094e6cf93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 'word' # {'word', 'phrases'}\n",
    "score_method = 'odds_ratio' # {'fscore','rank_difference', 'dense_rank', 'odds_ratio'}\n",
    "axes = 'score' # {'freq', 'score'}\n",
    "filename = f'{ngram}_method-{score_method}_axes-{axes}_compaction-{compaction}-freq-{minimum_term_frequency}'\n",
    "\n",
    "html = st.produce_fightin_words_explorer(\n",
    "    corpus_unigram,\n",
    "    category=category,\n",
    "    category_name=category,\n",
    "    not_category_name=not_category,\n",
    "    not_categories=[not_category],\n",
    "    neutral_categories=[],\n",
    "    transform=st.Scalers.percentile_alphabetical,\n",
    "    minimum_term_frequency=minimum_term_frequency,\n",
    "    term_scorer=st.LogOddsRatioInformativeDirichletPrior(priors_unigram, 10, 'class-size'),   # also try: st.LogOddsRatioInformativeDirichletPrior(priors, 1, 'corpus-size')\n",
    "    # metadata = rdf['movie_name'],\n",
    "    grey_threshold=0.1\n",
    ")\n",
    "\n",
    "open(output_dir+filename+'.html', 'w').write(html)\n",
    "\n",
    "\n",
    "# just copy it but add return_scatterplot_structure=True to output the structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f0f99-5db2-48e1-8351-691282015980",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 'word' # {'word', 'phrases'}\n",
    "score_method = 'odds_ratio' # {'fscore','rank_difference', 'dense_rank', 'odds_ratio'}\n",
    "axes = 'score' # {'freq', 'score'}\n",
    "filename = f'{ngram}_method-{score_method}_axes-{axes}_compaction-{compaction}-freq-{minimum_term_frequency}'\n",
    "\n",
    "\n",
    "scattertext_structure = st.produce_fightin_words_explorer(\n",
    "corpus_unigram,\n",
    "    category=category,\n",
    "    category_name=category,\n",
    "    not_category_name=not_category,\n",
    "    not_categories=[not_category],\n",
    "    neutral_categories=[],\n",
    "    transform=st.Scalers.percentile_alphabetical,\n",
    "    minimum_term_frequency=minimum_term_frequency,\n",
    "    term_scorer=st.LogOddsRatioInformativeDirichletPrior(priors_unigram, 10, 'class-size'),   # also try: st.LogOddsRatioInformativeDirichletPrior(priors, 1, 'corpus-size')\n",
    "    # metadata = rdf['movie_name'],\n",
    "    grey_threshold=0.1,\n",
    "\n",
    "    \n",
    "    # y_axis_values = [-1,0,1],\n",
    "    return_scatterplot_structure=True\n",
    ")\n",
    "# Change plotting params add '_' at the beginning:\n",
    "# scattertext_structure._x_label = 'Log frequency in entire corpus'\n",
    "# scattertext_structure._y_label = 'Non-suicidal vs. suicidal (Scaled F-Score)'\n",
    "\n",
    "# Change plotting params: I think it requires 3 values\n",
    "log_freq_distr = dispersion['Frequency'].values\n",
    "min_freq = int(np.min(log_freq_distr))\n",
    "max_freq = int(np.max(log_freq_distr))\n",
    "scattertext_structure._x_axis_labels=[min_freq, '',max_freq]\n",
    "# https://github.com/JasonKessler/scattertext/blob/b41e3a875faf6dd886e49e524345202432db1b21/scattertext/viz/PyPlotFromScattertextStructure.py#L149\n",
    "# scattertext_structure._y_axis_labels=[-1,0,1]#[f'{not_category} (-1)','0',f'{category} (1)']\n",
    "\n",
    "\n",
    "\n",
    "fig = st.produce_scattertext_pyplot(scattertext_structure, textsize = 8, dpi=300)\n",
    "# plt.xticks(ticks = [-1,0,1],labels=[-1,0,1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(output_dir+f'{filename}.tiff', format='tiff', dpi = 150)\n",
    "fig.savefig(output_dir+f'{filename}.png', format='png', dpi = 150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe509a7-24cc-44da-97c4-57420300448d",
   "metadata": {},
   "source": [
    "# Phrase scores\n",
    "\n",
    "Phrasemachine from AbeHandler (Handler et al. 2016) uses regular expressions over sequences of part-of-speech tags to identify noun phrases. This has the advantage over using spaCy's NP-chunking in that it tends to isolote meaningful, large noun phases which are free of appositives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10aa35b-0ee1-4ca9-9726-1fd1bd1315ef",
   "metadata": {},
   "source": [
    "## Phrase scores (axes are term frequency in each group, color-coded by F-scores)\n",
    "\n",
    "dense-rank difference: a score which is used in most of the two-category contrastive plots here\n",
    "\n",
    "maximum category-specific score: most prominent phrases in each category, regardless of the prominence in the other category\n",
    "\n",
    "Fscore: st.ScaledFScorePresetsNeg1To1(beta=1, scaler_algo='normcdf'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c04990-e76e-43d6-b9f3-dc965477b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispersion_phrases = st.Dispersion(corpus_phrases).get_df()\n",
    "dispersion_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a0d40-8be9-4910-bcd5-fde5a8181274",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 'phrases' # {'word', 'phrases'}\n",
    "score_method = 'fscore' # {'fscore','rank_difference', 'dense_rank', 'odds_ratio'}\n",
    "axes = 'freq' # {'freq', 'score'}\n",
    "\n",
    "minimum_term_frequency=10\n",
    "filename = f'{ngram}_method-{score_method}_axes-{axes}_compaction-{compaction}-freq-{minimum_term_frequency}'\n",
    "\n",
    "\n",
    "html = produce_scattertext_explorer(return_corpus(ngram),\n",
    "                                    category=category,\n",
    "                                    category_name=category,\n",
    "                                    not_category_name=not_category,\n",
    "                                    minimum_term_frequency=minimum_term_frequency,\n",
    "                                    pmi_threshold_coefficient=0,\n",
    "                                    transform=st.Scalers.percentile_alphabetical,\n",
    "                                        # grey_threshold=0.1,\n",
    "\n",
    "                                    # metadata=corpus.get_df()['speaker'],\n",
    "                                    term_scorer=st.ScaledFScorePresetsNeg1To1(beta=1, scaler_algo='normcdf'),\n",
    "                                    width_in_pixels=1000)\n",
    "\n",
    "open(output_dir+f'{filename}.html', 'w').write(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6acc927-03a0-4af9-8752-2d8fd43b27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_method(score_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28f27a-f33c-48b0-9d05-936310cd597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 'phrases' # {'word', 'phrases'}\n",
    "score_method = 'fscore' # {'fscore','rank_difference', 'dense_rank', 'odds_ratio'}\n",
    "axes = 'freq' # {'freq', 'score'}\n",
    "\n",
    "minimum_term_frequency=10\n",
    "filename = f'{ngram}_method-{score_method}_axes-{axes}_compaction-{compaction}-freq-{minimum_term_frequency}'\n",
    "\n",
    "# same as above with     return_scatterplot_structure=True\n",
    "scattertext_structure = st.produce_scattertext_explorer(\n",
    "    corpus_phrases,\n",
    "    category=category,\n",
    "    category_name=category,\n",
    "    not_category_name=not_category,\n",
    "    not_categories=[not_category],\n",
    "    neutral_categories=[],\n",
    "    transform=st.Scalers.percentile_alphabetical,\n",
    "    # transform=dense_rank,\n",
    "    minimum_term_frequency=0,\n",
    "    term_scorer=st.ScaledFScorePresetsNeg1To1(beta=1, scaler_algo='normcdf'),\n",
    "    # metadata = rdf['movie_name'],\n",
    "    # grey_threshold=0,\n",
    "\n",
    "    \n",
    "    # y_axis_values = [-1,0,1],\n",
    "    return_scatterplot_structure=True\n",
    ")\n",
    "# Change plotting params add '_' at the beginning:\n",
    "# scattertext_structure._x_label = 'Log frequency in entire corpus'\n",
    "# scattertext_structure._y_label = 'Non-suicidal vs. suicidal (Scaled F-Score)'\n",
    "\n",
    "# Change plotting params: I think it requires 3 values\n",
    "# log_freq_distr = dispersion['Frequency'].values\n",
    "# min_freq = int(np.min(log_freq_distr))\n",
    "# max_freq = int(np.max(log_freq_distr))\n",
    "# scattertext_structure._x_axis_labels=[min_freq, '',max_freq]\n",
    "# https://github.com/JasonKessler/scattertext/blob/b41e3a875faf6dd886e49e524345202432db1b21/scattertext/viz/PyPlotFromScattertextStructure.py#L149\n",
    "# scattertext_structure._y_axis_labels=[-1,0,1]#[f'{not_category} (-1)','0',f'{category} (1)']\n",
    "\n",
    "\n",
    "\n",
    "fig = st.produce_scattertext_pyplot(scattertext_structure, textsize = 8, dpi=300)\n",
    "# plt.xticks(ticks = [-1,0,1],labels=[-1,0,1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# fig.savefig(output_dir+f'{filename}.tiff', format='tiff', dpi = 150)\n",
    "# fig.savefig(output_dir+f'{filename}.png', format='png', dpi = 150)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f308d96-21cb-4888-897e-0622e7332f0f",
   "metadata": {},
   "source": [
    "## Phrase scores (axes are F-score as a function of term frequency in corpus, color-coded by F-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8b7bc-5642-494f-8b7f-dc60def14083",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5855c3-9d9d-46a9-84a0-9f7ec8fc68a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 'phrases' # {'word', 'phrases'}\n",
    "score_method = 'fscore' # {'fscore','rank_difference', 'dense_rank', 'odds_ratio'}\n",
    "axes = 'score' # {'freq', 'score'}\n",
    "\n",
    "minimum_term_frequency=0\n",
    "filename = f'{ngram}_method-{score_method}_axes-{axes}_compaction-{compaction}-freq-{minimum_term_frequency}'\n",
    "\n",
    "\n",
    "html = st.produce_fightin_words_explorer(\n",
    "    # return_corpus(ngram),\n",
    "    corpus_phrases,\n",
    "    filter_unigrams = False,\n",
    "    category=category,\n",
    "    category_name=category,\n",
    "    not_category_name=not_category,\n",
    "    not_categories=[not_category],\n",
    "    neutral_categories=[],\n",
    "    transform=st.Scalers.percentile_alphabetical,\n",
    "    minimum_term_frequency=minimum_term_frequency,\n",
    "    term_scorer=st.ScaledFScorePresetsNeg1To1(beta=1, scaler_algo='normcdf'),\n",
    "    # metadata = rdf['movie_name'],\n",
    "    # grey_threshold=0.1\n",
    ")\n",
    "\n",
    "open(output_dir+f'{filename}.html', 'w').write(html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372da04c-482d-48b6-9351-5d3b9a4e9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# html = st.produce_frequency_explorer(\n",
    "#     corpus.remove_terms(set(corpus.get_terms()) - set(term_freq_df.index)),\n",
    "#     category=category,\n",
    "# category_name=category,\n",
    "# not_category_name=not_category,\n",
    "#     not_categories=[not_category],\n",
    "#     term_scorer=st.ScaledFScorePresets(beta=1, one_to_neg_one=True),\n",
    "#     # metadata=rdf['movie_name'],\n",
    "#     grey_threshold=0\n",
    "# )\n",
    "\n",
    "# open(output_dir+f'{filename}.html', 'w').write(html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5eac01-3329-4ae2-b93f-4d81e66bc5b9",
   "metadata": {},
   "source": [
    "## Phrase scores (axes are weighted log odds ratio as a function of term frequency in corpus, color-coded by weighted log odds ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76857a48-a079-46a1-abac-08c98e675e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors_phrases = (st.PriorFactory(corpus_phrases, \n",
    "                  category=category, \n",
    "                  not_categories=[not_category],\n",
    "                  starting_count=0.01)\n",
    "  .use_neutral_categories()\n",
    "  .get_priors())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c637a9d-cd05-40f3-8564-4d76cff32770",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = 'phrases' # {'word', 'phrases'}\n",
    "score_method = 'fscore' # {'fscore','rank_difference', 'dense_rank', 'odds_ratio'}\n",
    "axes = 'freq' # {'freq', 'score'}\n",
    "\n",
    "minimum_term_frequency=0\n",
    "filename = f'{ngram}_method-{score_method}_axes-{axes}_compaction-{compaction}-freq-{minimum_term_frequency}'\n",
    "\n",
    "html = produce_scattertext_explorer(corpus_phrases,\n",
    "                                    category=category,\n",
    "                                    category_name=category,\n",
    "                                    not_category_name=not_category,\n",
    "                                    minimum_term_frequency=3,\n",
    "                                    pmi_threshold_coefficient=0,\n",
    "                                    transform=dense_rank,\n",
    "                                    # metadata=corpus.get_df()['speaker'],\n",
    "                                    # term_scorer=RankDifference(),\n",
    "                                    # term_scorer= st.ScaledFScorePresetsNeg1To1(beta=1, scaler_algo='normcdf'),\n",
    "                                    term_scorer=st.LogOddsRatioInformativeDirichletPrior(priors_phrases, 10, 'class-size'),    \n",
    "                                    width_in_pixels=1000)\n",
    "\n",
    "open(output_dir+f'{filename}.html', 'w').write(html)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4c80e-4ef4-4c3b-9bca-cb97cd4bc734",
   "metadata": {},
   "source": [
    "## Phrase scores using a different phrase-detection method textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b1f922-099a-4b69-9eef-be7b352c22d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Alternative phrase detection method \n",
    "\n",
    "%%time \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "nlp.add_pipe(\"textrank\", last=True)\n",
    "\n",
    "df = train.assign(\n",
    "    parse=lambda df: df[text_col_name].apply(nlp),\n",
    "    # party=lambda df: df[group_col_name].apply({'democrat': 'Democratic', 'republican': 'Republican'}.get)\n",
    ")\n",
    "corpus = st.CorpusFromParsedDocuments(\n",
    "    df,\n",
    "    category_col=group_col_name,\n",
    "    parsed_col='parse',\n",
    "    feats_from_spacy_doc=st.PyTextRankPhrases()\n",
    ").build(\n",
    ").compact(\n",
    "    AssociationCompactor(2000, use_non_text_features=True) # number of phrases displayed in the chart\n",
    "\n",
    ")\n",
    "# The phrases generated will be treated like non-textual features since their document scores will not correspond to word counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce51b4-bf6b-495a-abd7-b0fec4dddf74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "term_ranks = np.argsort(np.argsort(-term_category_scores, axis=0), axis=0) + 1\n",
    "metadata_descriptions = {\n",
    "    term: '<br/>' + '<br/>'.join(\n",
    "        '<b>%s</b> TextRank score rank: %s/%s' % (cat, term_ranks.loc[term, cat], corpus.get_num_metadata())\n",
    "        for cat in corpus.get_categories())\n",
    "    for term in corpus.get_metadata()\n",
    "}\n",
    "category_specific_prominence = term_category_scores.apply(\n",
    "    # lambda r: r.Democratic if r.Democratic > r.Republican else -r.Republican,\n",
    "    lambda r: r['Suicidal'] if r['Suicidal'] > r['Non-suicidal'] else -r['Non-suicidal'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a85fe-fde7-4764-b85e-ce4009d90475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "html = produce_scattertext_explorer(\n",
    "    corpus,\n",
    "    category=category,\n",
    "    not_category_name=not_category,\n",
    "    minimum_term_frequency=2,\n",
    "    pmi_threshold_coefficient=0,\n",
    "    width_in_pixels=1000,\n",
    "    transform=dense_rank,\n",
    "    # metadata=corpus.get_df()['speaker'],\n",
    "    scores=category_specific_prominence,\n",
    "    sort_by_dist=False,\n",
    "    use_non_text_features=True,\n",
    "    topic_model_term_lists={term: [term] for term in corpus.get_metadata()},\n",
    "    topic_model_preview_size=0,\n",
    "    metadata_descriptions=metadata_descriptions,\n",
    "    use_full_doc=True\n",
    ")\n",
    "open(output_dir+'explorer_collocations.html', 'w').write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bf8f3-265e-4b05-a605-58c03ea4a518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "html = produce_scattertext_explorer(\n",
    "    corpus,\n",
    "    category=category,\n",
    "    not_category_name=not_category,\n",
    "    minimum_term_frequency=2,\n",
    "    pmi_threshold_coefficient=0,\n",
    "    width_in_pixels=1000,\n",
    "    transform=dense_rank,\n",
    "    use_non_text_features=True,\n",
    "    # metadata=corpus.get_df()['speaker'],\n",
    "    term_scorer=RankDifference(),\n",
    "    sort_by_dist=False,\n",
    "    topic_model_term_lists={term: [term] for term in corpus.get_metadata()},\n",
    "    topic_model_preview_size=0, \n",
    "    metadata_descriptions=metadata_descriptions,\n",
    "    use_full_doc=True\n",
    ")\n",
    "open(output_dir+'explorer_collocations_2.html', 'w').write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee1fb4-3b72-4a20-bc60-74a67237d179",
   "metadata": {},
   "source": [
    "# How do the two types of post talk about suicide?\n",
    "\n",
    "In this configuration of Scattertext, words are colored by their similarity to a query phrase.\n",
    "This is done using spaCy-provided GloVe word vectors (trained on the Common Crawl corpus). The cosine distance between vectors is used, with mean vectors used for phrases.\n",
    "\n",
    "The calculation of the most similar terms associated with each category is a simple heuristic. First, sets of terms closely associated with a category are found. Second, these terms are ranked based on their similarity to the query, and the top rank terms are displayed to the right of the scatterplot.\n",
    "\n",
    "A term is considered associated if its p-value is less than 0.05. P-values are determined using Monroe et al. (2008)'s difference in the weighted log-odds-ratios with an uninformative Dirichlet prior. This is the only model-based method discussed in Monroe et al. that does not rely on a large, in-domain background corpus. Since we are scoring bigrams in addition to the unigrams scored by Monroe, the size of the corpus would have to be larger to have high enough bigram counts for proper penalization. This function relies the Dirichlet distribution's parameter alpha, a vector, which is uniformly set to 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3a6f3-b39c-418e-a42d-8618792633d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scattertext import word_similarity_explorer\n",
    "html = word_similarity_explorer(corpus_unigram,\n",
    "                                 category=category,\n",
    "                                 category_name=category,\n",
    "                                 not_category_name=not_category,\n",
    "                                 target_term='suicide',\n",
    "                                 minimum_term_frequency=minimum_term_frequency,\n",
    "                                 pmi_threshold_coefficient=0,\n",
    "                                 width_in_pixels=1000,\n",
    "                                \n",
    "                                 # metadata=convention_df['speaker'],\n",
    "                                 alpha=0.01,\n",
    "                                 max_p_val=0.05,\n",
    "                                 save_svg_button=True)\n",
    "open(output_dir+\"explorer_suicide.html\", 'wb').write(html.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b6f63-d413-4777-8e91-a486f77312a4",
   "metadata": {},
   "source": [
    "# Maybe also compare to My other implementation using convokit code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92991ff3-b51f-404a-93e1-1ede0fad0b37",
   "metadata": {},
   "source": [
    "# Explore with scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2283d0-e51a-4473-9fff-fc8f4650a092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "method_name = 'fscore'\n",
    "if method_name == 'fscore':\n",
    "    method = st.ScaledFScorePresets(beta=1, one_to_neg_one=True)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from scattertext import sparse_explorer\n",
    "html = sparse_explorer(corpus_unigram,\n",
    "                        category=category,\n",
    "                        category_name=not_category,\n",
    "                        not_category_name=not_category,\n",
    "                        # scores = corpus.get_regression_coefs('democrat', Lasso(max_iter=10000)),\n",
    "                           scores = method,\n",
    "                        minimum_term_frequency=minimum_term_frequency,\n",
    "                        pmi_threshold_coefficient=4,\n",
    "                        width_in_pixels=1000,\n",
    "                        # metadata=convention_df['speaker']\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a4f3c-a672-4a46-a667-ac97fc0d423d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# semiotic square \n",
    "\n",
    "You need a neutral group (if comparing positive and negative opion sentiments, plot descriptions could be neutral)\n",
    "\n",
    "https://github.com/JasonKessler/scattertext#creating-lexicalized-semiotic-squares\n",
    "\n",
    "http://www.signosemio.com/greimas/semiotic-square.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ba236-1bbe-4eb2-a876-dd41441d348b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "rdf = st.SampleCorpora.RottenTomatoes.get_data() #let's use rotten tomato which has neutral categories\n",
    "rdf['category_name'] = rdf['category'].apply(lambda x: {'plot': 'Plot', 'rotten': 'Negative', 'fresh': 'Positive'}[x])\n",
    "rdf['parse'] = rdf.text.apply(nlp)\n",
    "\n",
    "phrase_corpus = (st.CorpusFromParsedDocuments(rdf, \n",
    "                                       category_col='category_name', \n",
    "                                       parsed_col='parse',\n",
    "                                       feats_from_spacy_doc = st.PhraseMachinePhrases())\n",
    "          .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d235c22-5e6c-41cb-ad01-fc0eda3bcb11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category = 'Positive'\n",
    "not_category = 'Negative'\n",
    "neutral_category_name = 'Plot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd7e85-de1d-4c64-90d8-cf91e52686fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phrase_corpus_compact = corpus_pm.compact(st.CompactTerms(minimum_term_count=2))\n",
    "\n",
    "semiotic_square = st.SemioticSquare(\n",
    "    phrase_corpus_compact,\n",
    "    category_a=category,\n",
    "    category_b=not_category,\n",
    "    neutral_categories=[neutral_category_name],\n",
    "    scorer=st.RankDifference(), \n",
    "    labels = {'a_and_b': 'Posts',\n",
    "              'not_a_and_not_b': 'Plot Descriptions',\n",
    "              'a_and_not_b': f'{category}/Plot',\n",
    "              'b_and_not_a': f'{not_category}/Plot',\n",
    "             }\n",
    ")\n",
    "\n",
    "html = st.produce_semiotic_square_explorer(semiotic_square,\n",
    "                                           category_name=category,\n",
    "                                           not_category_name=not_category,\n",
    "                                           x_label=f'{category}-{not_category}',\n",
    "                                           y_label=f'Post-{neutral_category_name}',\n",
    "                                           minimum_term_frequency=2,\n",
    "                                           pmi_threshold_coefficient=0,\n",
    "                                           neutral_category_name='Plot Description',\n",
    "                                           # metadata=rdf['movie_name']\n",
    "                                          )\n",
    "\n",
    "open(output_dir+'semiotic_square_1.html', 'w').write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d7f56-3baf-44a2-85bc-cf27e602e2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
